---
title: "State impact of SALT-cap repeal"
author: "Don Boyd and Matt Jensen"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook: 
    df_print: paged
    fig_height: 6
    fig_width: 8
    toc: yes
    number_sections: yes
editor_options:
  chunk_output_type: inline
---

# Intro notes

-   Go to the end to see QC analysis, issues, etc.

```{r setup, eval=TRUE, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# note that eval=TRUE unless set to FALSE
knitr::opts_chunk$set(eval=TRUE, include=FALSE, echo = FALSE)
options(width = 150)
```


```{r git_info}
# link to github
# see DONOTCOMMIT.txt for my personal access token

# • To create a personal access token, call `create_github_token()`
# • To store a token for current and future use, call `gitcreds::gitcreds_set()`
# ℹ Read more in the 'Managing Git(Hub) Credentials' article:
#   https://usethis.r-lib.org/articles/articles/git-credentials.html

# usethis::gh_token_help()
# gitcreds::gitcreds_set()

# then, IF NOT ALREADY DONE, create a repo on github:
# usethis::use_github()

```


```{r libraries}
library(tidyverse)
options(tibble.print_max = 80, tibble.print_min = 80) # if more than 60 rows, print 60 - enough for states
library(arrow)
library(kableExtra)
library(btools)
library(gt)
library(knitr)

library(maps)
# https://cran.r-project.org/web/packages/usmap/vignettes/mapping.html
library(usmap)
library(gridExtra)
library(RcppRoll)
library(ggbreak)
library(patchwork)
library(RColorBrewer)


# https://cran.r-project.org/web/packages/arrow/vignettes/install.html
# install.packages("arrow", repos = "https://arrow-r-nightly.s3.amazonaws.com")
# install_arrow(nightly=TRUE)  # from the script in this directory

# Sys.setenv(
#       LIBARROW_DOWNLOAD = "true",
#       LIBARROW_BINARY = binary,
#       LIBARROW_MINIMAL = minimal,
#       ARROW_R_DEV = verbose,
#       ARROW_USE_PKG_CONFIG = use_system
#     )

# Sys.getenv("LIBARROW_MINIMAL", FALSE)
# Sys.getenv("LIBARROW_MINIMAL")


```


```{r locations}

os <- "win"  # win or linux

if(os=="win"){
  SCRATCHDIR <- r"(E:\linux\scratch\)"
  WEIGHTDIR <- r"(E:\linux\pufanalysis_output\weights\)"
} else if(os=="linux"){
  SCRATCHDIR <- '/media/don/scratch/'
  DATADIR <- '/media/don/pufanalysis_output/data/'
  WEIGHTDIR <- '/media/don/pufanalysis_output/weights/'
}


```


```{r constants}
geos <- c(state.abb, "other")

# df with upper bound of open intervals (does not include endpoint on right)
agi_stubs <- tribble(~agicut, ~agilabel,
                     -Inf, "Negative",
                     1, "Under $1",
                     25e3, "$1 to < $25k",
                     50e3, "$25k < 50k",
                     75e3, "$50k < 75k",
                     100e3, "$75k < $100k",
                     200e3, "$100k < $200k",
                     500e3, "$200k < $500k",
                     1e6, "$500k < $1m",
                     Inf, "$1m+")
# The following will cut an agi variable with the endpoints of each interval set properly, right=FALSE:
# cut(c00100, agi_stubs$agicut, labels=agi_stubs$agilabel[-1], include.lowest = TRUE, right=FALSE)
# test
# agi <- c(-10, 0, 1, 2, 24999, 25000, 25001, 199999, 200000, 200001, 2e6)
# tibble(agi=agi) %>% 
#   mutate(stub=cut(agi, agi_stubs$agicut, labels=agi_stubs$agilabel[-1], include.lowest = TRUE, right=FALSE))

```


```{r utility_functions}

get_stname <- function(stabbr){
  allgeos <- c("US", geos)
  allnames <- c("United States", state.name, "Other")
  allnames[match(stabbr, allgeos)]
}
# get_stname("AK")
# get_stname("US")
# get_stname(state.abb)


```


```{r get_weights, include=FALSE}
# get national weights, state weights and create state shares,

stweights2017 <- read_csv(paste0(WEIGHTDIR, 'allweights2017_geo_restricted.csv')) %>%
  mutate(RECID=pid + 1)

glimpse(stweights2017)

stshares <- stweights2017 %>%
  mutate(across(all_of(geos), ~ .x / geoweight_sum))

glimpse(stshares)

# do all shares add to 1?
check <- stshares %>%
  pivot_longer(cols=all_of(geos)) %>%
  group_by(RECID) %>%
  summarise(sum=sum(value), .groups="drop")
check %>% filter(abs(sum - 1) > 1e-6)  

usweights <- read_csv(paste0(SCRATCHDIR, 'weights2021.csv'))
glimpse(usweights)
usweights %>%
  group_by(filer2017) %>%
  summarise(across(-c(RECID), ~ sum(.x))) %>%
  add_row() %>%
  mutate(across(-filer2017, ~ifelse(is.na(filer2017), sum(.x, na.rm=TRUE), .x)))

# create state weights for both sets of 2021 national weights
stweights2021 <- usweights %>%
  select(RECID, filer2017, WT2021, REWT2021) %>%
  pivot_longer(cols=c(WT2021, REWT2021),
               names_to = "wtype", values_to = "usweight") %>%
  left_join(stshares %>% select(RECID, all_of(geos)), by="RECID") %>%
  mutate(across(all_of(geos), ~ .x * usweight))
glimpse(stweights2021) # note that this gives us NA state weights for nonfilers; eventually we'll have nonfiler weights

# verify that state weights sum to national weights for each record
check <- stweights2021 %>%
  filter(filer2017) %>%
  select(RECID, wtype, usweight, all_of(geos)) %>%
  pivot_longer(-c(RECID, wtype, usweight)) %>%
  group_by(RECID, wtype, usweight) %>%
  summarise(wtsum=sum(value), .groups="drop") %>%
  mutate(diff=wtsum - usweight,
         pdiff=diff / usweight * 100)
# quantile(check$pdiff)  # should be very near zero

# Are any weights negative??
# summary(stweights2021)  # no

```


```{r get_tcoutput}

vars <- c("RECID", "c00100", "e00200",  "standard", "c04470", "c18300",  "iitax")

basedf <- read_parquet(paste0(SCRATCHDIR, "base2021.parquet"),
                       col_select = all_of(vars)) %>%
  mutate(agibase=c00100, type="base")

reformdf <- read_parquet(paste0(SCRATCHDIR, "reform2021.parquet"),
                       col_select = all_of(vars)) %>%
  left_join(basedf %>% select(RECID, agibase), by="RECID") %>% mutate(type="reform")


```


```{r include=FALSE}
# put base agi on reform file, stack the files, add US reweight, add agi group

stack <- bind_rows(basedf, reformdf) %>%
  left_join(stweights2021, by="RECID") %>%
  mutate(stub=cut(agibase, agi_stubs$agicut, labels=agi_stubs$agilabel[-1], include.lowest = TRUE, right=FALSE),
         stubnum=as.integer(stub))

glimpse(stack)  


```


```{r prep_analysis, include=FALSE}
# create a file that has state weights with original puf weights and with my puf weights

wstack <- stack %>%
  mutate(across(c(usweight, all_of(geos)),
                list(agi=~.x * c00100,
                     iitax=~.x * iitax,
                     salt=~.x * c18300)))
glimpse(wstack) # ~1m recs, variables are weighted

wsums <- wstack %>%
  filter(filer2017) %>%
  select(-c(RECID, filer2017, c00100, e00200, c18300, iitax, agibase)) %>%
  group_by(type, wtype, stubnum, stub) %>%
  summarise(across(.cols=everything(), sum), .groups="drop")
glimpse(wsums) # 36 rows type 2, wtype 2, stubs 9, each col is weighted value for variable, state
count(wsums, type, wtype, stubnum, stub)
ns(wsums)

wsums_long <- wsums %>%
  pivot_longer(-c(type, wtype, stubnum, stub))

# add in the all-stubs sum for each group
stubsums <- wsums_long %>%
  group_by(type, wtype, name) %>%
  summarise(value=sum(value, na.rm=TRUE), .groups="drop") %>%
  mutate(stubnum=0, stub="Total")

allsums <- wsums_long %>%
  mutate(stub=as.character(stub)) %>%
  bind_rows(stubsums) %>%
  mutate(name=ifelse(!str_detect(name, "_"), paste0(name, "_numret"), name),
         name=str_replace(name, "usweight", "US")) %>%
  separate(name, into=c("stabbr", "variable")) %>%
  arrange(wtype, type, stabbr, variable, stubnum)
glimpse(allsums)
count(allsums, type)
count(allsums, wtype)
count(allsums, stubnum, stub)
count(allsums, variable)
count(allsums, stabbr)

# iris %>%
#   group_by(Species) %>%
#   summarise(across(starts_with("Sepal"), list(mean, sd), .names = "{.col}.fn{.fn}"))

# iris %>%
#   group_by(Species) %>%
#   summarise(across(starts_with("Sepal"), list(mean = mean, sd = sd), .names = "{.col}.{.fn}"))

```


```{r}
# selected functions
getvar <- function(wtype, stabbr, variable){
  allsums %>%
    filter(wtype==!!wtype, stabbr==!!stabbr, variable==!!variable) %>%
    pivot_wider(names_from = type) %>%
    mutate(change=reform - base,
         pchange=change / base) %>%
    select(wtype, stabbr, variable, everything())
}


# getvar(wtype="REWT2021", stabbr="US", variable="iitax")
# getvar(wtype="WT2021", stabbr="US", variable="iitax")
# getvar(wtype="REWT2021", stabbr="MS", variable="iitax")

vartab <- function(tabdata, title, dollar_digits=1){
  dcols <- c("base", "reform", "change")
  tabdata %>%
    gt()  %>%  
    tab_header(
      title = title,
      subtitle = "Billions of dollars"
    ) %>%
    fmt_currency(
      columns = all_of(dcols),
      rows=1,
      decimals = dollar_digits,
      scale_by = 1e-9,
      suffixing = FALSE
    )   %>%
    fmt_number(
      columns = all_of(dcols),
      rows=2:nrow(tabdata),
      decimals = dollar_digits,
      scale_by = 1e-9,
      suffixing = FALSE
    ) %>%
    fmt_percent(
      columns = "pchange",
      decimals = 1
    ) %>%
    tab_style(
      style = list(
        cell_fill(color = "#f7f7f7")
      ),
      locations = cells_body(
        rows = seq(1, nrow(tabdata), 2)
        )
    )
}


```


# Possible tables and graphs for the report

## National analysis

```{r}
# data3 is the key result

str(knitr::opts_chunk$get()) # default chuck options
# opts_current$get()
allsums  # "type"     "wtype"    "stubnum"  "stub"     "stabbr"   "variable" "value"  
count(allsums, variable)

# create a file that has number of returns, value, and average value for each rec, with reweights
data1 <- allsums %>%
  filter(wtype=="REWT2021") %>%
  rename(wtdsum=value) %>%
  group_by(type, wtype, stabbr, stub, stubnum) %>%
  mutate(numret=wtdsum[variable=="numret"],
         avgval=wtdsum / numret) %>%
  ungroup

data2 <- data1 %>%
  pivot_longer(c(wtdsum, numret, avgval), names_to = "measure") %>%
  pivot_wider(names_from=type, values_from=value) %>%
  mutate(change=reform - base,
         pchange=ifelse(base > 0, change / base, NA_real_))

data3 <- data2 %>%
  left_join(data2 %>% 
              filter(stabbr=="US") %>% 
              select(wtype, stubnum, stub, variable, measure, us_base=base, us_reform=reform, us_change=change, us_pchange=pchange),
            by=c("wtype", "stubnum", "stub", "variable", "measure"))


```


```{r ustable, include=TRUE}

tabdata <- data3 %>%
  filter(stabbr=="US", measure %in% c("wtdsum", "avgval"), variable=="iitax") %>%
  select(stubnum, stub, measure, base, reform, change, pchange) %>%
  pivot_longer(-c(stubnum, stub, measure)) %>%
  unite("name", c(measure, name)) %>%
  pivot_wider() %>%
  mutate(junk="") %>%
  arrange(stubnum) %>%
  select(stub, starts_with("wtdsum"), junk, starts_with("avg"))

dcols <- c("wtdsum_base", "wtdsum_reform", "wtdsum_change", "avgval_base", "avgval_reform", "avgval_change")
pcols <- c("wtdsum_pchange", "avgval_pchange")

tab <- tabdata %>%
  gt()  %>%  
  tab_header(
    title = "Impact of SALT-cap repeal for the United States",
    subtitle = "Compared to current 2021 law"
  ) %>%
  cols_label(
      stub = "",
      wtdsum_base = "2021 current law",
      wtdsum_reform = "SALT-cap repeal",
      wtdsum_change = html("$ change: <br>Repeal minus<br>current law"),
      wtdsum_pchange = html("% change: <br>Repeal versus<br>current law"),
      
      junk="",
      
      avgval_base = "2021 current law",
      avgval_reform = "SALT-cap repeal",
      avgval_change = html("$ change: <br>Repeal minus<br>current law"),
      avgval_pchange = html("% change: <br>Repeal versus<br>current law")
    ) %>%
  
  fmt_currency(
    columns = all_of(str_subset(dcols, "wtdsum")),
    rows=1,
    decimals = 1,
    scale_by = 1e-9,
    suffixing = FALSE
  )   %>%
  fmt_currency(
    columns = all_of(str_subset(dcols, "avgval")),
    rows=1,
    decimals = 0,
    scale_by = 1,
    suffixing = FALSE
  )   %>%
  fmt_number(
    columns = all_of(str_subset(dcols, "wtdsum")),
    rows=2:nrow(tabdata),
    decimals = 1,
    scale_by = 1e-9,
    suffixing = FALSE
  ) %>%
  fmt_number(
    columns = all_of(str_subset(dcols, "avgval")),
    rows=2:nrow(tabdata),
    decimals = 0,
    scale_by = 1,
    suffixing = FALSE
  ) %>%
  
  fmt_percent(
    columns = pcols,
    decimals = 1
  ) %>%
  fmt_missing(
    columns=pcols,
    missing_text = "--") %>%
  tab_style(
    style = list(
      cell_fill(color = "#f7f7f7")
    ),
    locations = cells_body(
      rows = seq(1, nrow(tabdata), 2)
      )
  ) %>%
  tab_spanner(
    label = html("Total impact in $ billions"),
    columns = c("wtdsum_base", "wtdsum_reform", "wtdsum_change", "wtdsum_pchange",)
  ) %>%
  tab_spanner(
    label = html("Average impact per return, in dollars"),
    columns = c("avgval_base", "avgval_reform", "avgval_change", "avgval_pchange")
  ) %>%
  cols_width(
    stub ~ px(150),
    starts_with("wtd") ~ px(100),
    junk ~ px(20),
    starts_with("avg") ~ px(100),
    everything() ~ px(60)
  )

tab

# configure save settings for wide table
gtsave(tab, "USImpact_table.png", path = here::here("results"), zoom=2, vwidth=1500)  # zoom 2 default


```


## State analysis

### Table of states ranked by $ billions change in iitax, base to reform

```{r include=TRUE}
tabdata <- data3 %>%
  filter(stubnum==0, variable=="iitax", measure=="wtdsum") %>%
  filter(stabbr %in% c("US", state.abb)) %>%
  mutate(stname=get_stname(stabbr)) %>%
  arrange(change) %>%
  select(stname, stname, base, reform, change, pchange)



tab <- tabdata %>%
  gt()  %>%  
  tab_header(
    title = "States ranked by impact of SALT-cap repeal, $ billions",
    subtitle = "Compared to current 2021 law"
  ) %>%
  cols_label(
      base = "2021 current law",
      reform = "SALT-cap repeal",
      change = html("$ change: <br>Repeal minus<br>current law"),
      pchange = html("% change: <br>Repeal versus<br>current law")
    ) %>%
  fmt_currency(
    columns = c("base", "reform", "change"),
    rows=1,
    decimals = 1,
    scale_by = 1e-9,
    suffixing = FALSE
  )   %>%
  fmt_number(
    columns = c("base", "reform", "change"),
    rows=2:nrow(tabdata),
    decimals = 1,
    scale_by = 1e-9,
    suffixing = FALSE
  ) %>%
  fmt_percent(
    columns = "pchange",
    decimals = 1
  ) %>%
  fmt_missing(
    columns="pchange",
    missing_text = "--") %>%
  tab_style(
    style = list(
      cell_fill(color = "#f7f7f7")
    ),
    locations = cells_body(
      rows = seq(1, nrow(tabdata), 2)
      )
  )
tab

# configure save settings for wide table
gtsave(tab, "states_ranked_table.png", path = here::here("results"), zoom=2, vwidth=1500)  # zoom 2 default



```



### Map of average change in iitax, all returns (not just itemizers, not just those with tax change)

```{r}
# map of $ change of all returns
theme_map <- function(base_size=9, base_family="") {
  # see:
  # https://socviz.co/maps.html
  # https://github.com/kjhealy/socviz
  require(grid)
  theme_bw(base_size=base_size, base_family=base_family) %+replace%
    theme(axis.line=element_blank(),
          axis.text=element_blank(),
          axis.ticks=element_blank(),
          axis.title=element_blank(),
          panel.background=element_blank(),
          panel.border=element_blank(),
          panel.grid=element_blank(),
          panel.spacing=unit(0, "lines"),
          plot.background=element_blank(),
          legend.justification = c(0,0),
          legend.position = c(0,0)
    )
}

get_mdata <- function(data){
  # df must have a column named stabbr
  mdata <- left_join(usmap::us_map() %>% arrange(full, piece, order),
                     data %>% rename(abbr=stabbr),
                     by="abbr")
  return(mdata)
}

get_stateplot <- function(data, fillvar){
  # data is a data frame
  # fillvar is a character value with the name of the variable that will determine the fill color
  mdata <- get_mdata(data)
  # p <- ggplot(data = mdata, aes(x = long, y = lat, group = group)) +
  p <- ggplot(data = mdata, aes(x = x, y = y, group = group)) +
    geom_polygon(aes(fill=.data[[fillvar]]), color = "gray90", size = 0.1) +
    coord_equal() +
    scale_colour_manual(values=c("black", "blue", "green", "red", "yellow"), na.translate = FALSE)  +
    theme_map() +
    theme(legend.position = "right")
  return(p)
}


```


```{r map, include=TRUE}
mapdata <- data3 %>%
  filter(stabbr %in% state.abb, stubnum==0, variable=="iitax", measure=="avgval") %>%
  select(stabbr, variable, measure, base, reform, change, us_change)

# mapdata %>% 
#   arrange(change)

md2 <- mapdata %>%
  mutate(valgroup=cut(change, 5))

md2 <- mapdata %>%
  mutate(valgroup=cut(change, 4))
# quantile(mapdata$change)

md2 <- mapdata %>%
  mutate(change=-change,
         valgroup=cut(change, breaks=c(-Inf, 200, 500, 1000, Inf), right=FALSE),
         valnum=as.integer(valgroup),
         vlabs = factor(valnum, 
                        levels=1:4, 
                        labels=c("< $200",
                                 "$200 < $500",
                                 "$500 < $1,000",
                                 ">= $1,000")))
# count(md2, valnum, valgroup, vlabs)

# p <- get_stateplot(md2, "valgroup")
# p + ggtitle("Repeal cap on SALT deduction",
#             subtitle="Change in average federal income tax per return")+
#    theme(legend.title = element_blank())

mdata <- get_mdata(md2)

cols <- brewer.pal(5, "RdBu")[-3]
p <- ggplot(data = mdata, aes(x = x, y = y, group = group)) +
  geom_polygon(aes(fill=.data[["vlabs"]]), color = "gray90", size = 0.1) +
  coord_equal() +
  # scale_fill_manual(values=brewer.pal(5, "Blues")[-1], na.translate=FALSE)  +
  scale_fill_manual(values=cols, na.translate=FALSE)  +
  theme_map() +
  theme(legend.position = "right") +
  theme(legend.title = element_blank()) + 
  ggtitle("Impact of repealing the SALT cap, change in average federal income tax per return",
          subtitle="Compared to current 2021 law")

p
ggsave(plot=p, filename=here::here("results", "avg_iitax_change_map.png"), width=8, height=6.5, units="in")

```



### States $ billions -- horizontal bars

```{r include=TRUE, fig.width=8, fig.height=6.5}
df <- data3 %>%
  filter(!stabbr %in% c("US", "other"), stubnum==0, measure=="wtdsum", variable=="iitax") %>%
  select(stabbr, change)


p <- df %>% 
  mutate(change=-change / 1e9,
         stname=get_stname(stabbr)) %>%
  ggplot(aes(y=reorder(stname, change), x=change)) +
  geom_col(fill="blue", width = 0.7) +
  scale_y_discrete(name=NULL) +
  scale_x_continuous(name="Aggregate tax cut, $ billions", labels = scales::dollar_format()) +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0)) +
  ggtitle("Impact of repealing the SALT cap, billions of dollars",
          subtitle="Compared to current 2021 law")
p
ggsave(plot=p, filename=here::here("results", "iitax_change_hbars.png"), width=8, height=6.5, units="in")

# df %>% 
#   mutate(change=-change) %>%
#   ggplot(aes(x=reorder(stabbr, change), y=change)) +
#   geom_col(fill="blue", width = 0.7) +
#   coord_flip()


```


### States average $ change per taxpayer -- horizontal bars


```{r include=TRUE, fig.width=8, fig.height=6.5}
df <- data3 %>%
  filter(!stabbr %in% c("US", "other"), stubnum==0, measure=="avgval", variable=="iitax") %>%
  select(stabbr, change)


p <- df %>% 
  mutate(change=-change,
         stname=get_stname(stabbr)) %>%
  ggplot(aes(y=reorder(stname, change), x=change)) +
  geom_col(fill="blue", width = 0.7) +
  scale_y_discrete(name=NULL) +
  scale_x_continuous(name="Average tax reduction per return", labels = scales::dollar_format()) +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0)) +
  ggtitle("Impact of repealing the SALT cap, averaged over all filers",
          subtitle="Compared to current 2021 law")
p
ggsave(plot=p, filename=here::here("results", "iitax_avgchange_hbars.png"), width=8, height=6.5, units="in")


```



### States scatterplot

```{r scatter, include=TRUE, fig.width=8, fig.height=6}

pdata <- data3 %>%
  filter(stabbr != "other", stubnum > 1, variable=="iitax", measure=="avgval") %>%
  select(stabbr, stubnum, stub, change) %>%
  mutate(us=stabbr=="US", change=-change) %>%
  group_by(us, stubnum, stub) %>%
  arrange(desc(change)) %>%
  mutate(rank=row_number()) %>%
  mutate(show=(rank <= 5) | (rank >= 46),
         group=case_when(stabbr=="US" ~ "US",
                         rank <= 5 ~ "bigcut",
                         rank >= 46 ~ "smallcut",
                         TRUE ~ "mid"),
         group=factor(group, levels=c("US", "bigcut", "mid", "smallcut"))) %>%
  ungroup

# count(pdata, us, show, stub)
# count(pdata, stub, group)

p <- pdata %>%
  filter(stubnum >= 6) %>%
  mutate(stlabel=ifelse(group=="mid", "", stabbr)) %>%
  ggplot(aes(x=reorder(stub, stubnum), y=change, label=stlabel, colour=group)) +
    geom_point(size=0.5) +
    geom_text(size=2) +
    # geom_text(fontface="bold", nudge_x=nudgex(), nudge_y=nudgey(), size=1.75, check_overlap = TRUE) +
    # geom_text_repel(size=2, nudge_x=0.15) +
    geom_hline(yintercept = 0, colour="black", size=0.1) +
    scale_colour_manual(values=c("darkgreen", "blue", "lightgrey", "red")) +
    scale_x_discrete(name="Adjusted gross income range") +
    scale_y_continuous(name="Average tax cut ($)", labels = scales::dollar_format()) +
    ggtitle("SALT-cap repeal compared to 2021 current law",
            subtitle="Selected adjusted gross income ranges") +
    theme_bw() +
    theme(legend.position = "none")

p
ggsave(filename=here::here("results", "taxcut_agirange_scatter.png"), plot=p, width=9, height=6, scale=1)


```






# Appendix: State-specific tables (not for the report)


```{r}

statetab <- function(stabbr){
  tabdata <- data3 %>%
    filter(stabbr==!!stabbr, measure=="avgval", variable=="iitax") %>%
    arrange(stubnum) %>%
    select(stub, base:us_pchange, -us_base, -us_reform)
  
  dcols <- c("base", "reform", "change", "us_change")
  pcols <- c("pchange", "us_pchange")
  stname <- get_stname(stabbr)
  tabdata %>%
    gt()  %>%  
    tab_header(
      title = paste0("Impact of SALT-cap repeal on ", stname),
      subtitle = "Compared to current 2021 law"
    ) %>%
   cols_label(
      stub = "",
      base = "2021 current law",
      reform = "SALT-cap repeal",
      change = html("$ change: <br>Repeal minus<br>current law"),
      pchange = html("% change: <br>Repeal versus<br>current law"),
      us_change = html("$ change: <br>Repeal minus<br>current law"),
      us_pchange = html("% change: <br>Repeal versus<br>current law"),
    ) %>%
    fmt_currency(
      columns = all_of(dcols),
      rows=1,
      decimals = 0,
      suffixing = FALSE
    )   %>%
    fmt_number(
      columns = all_of(dcols),
      rows=2:nrow(tabdata),
      decimals = 0,
      suffixing = FALSE
    ) %>%
    fmt_percent(
      columns = pcols,
      decimals = 1
    ) %>%
    fmt_missing(
      columns=pcols,
      missing_text = "--") %>%
    tab_style(
      style = list(
        cell_fill(color = "#f7f7f7")
      ),
      locations = cells_body(
        rows = seq(1, nrow(tabdata), 2)
        )
    ) %>%
    tab_spanner(
      label = html(paste0("Impact in ", stname)),
      columns = c("base", "reform", "change", "pchange")
    ) %>%
    tab_spanner(
      label = html(paste0("Impact in the U.S.")),
      columns = c("us_change", "us_pchange")
    )
}


state_report <- function(st){
  stname <- get_stname(st)
 
  cat("\n")
  cat("## ", stname, " \n")
 
  print(statetab(st))
  plot.new()
  dev.off()
 
  cat('\n\n')
}

```


```{r include=TRUE, results = 'asis'}

# state_report("AK")

purrr::map(state.abb, state_report)

# for(st in c("AL", "NY")){
#   state_report(st)
# }

```



# QC analysis starts here

## Records with tax increases, weighted by default puf weights

```{r include=TRUE}
# do any records have tax increases
increases <- stack %>%
  filter(wtype=="WT2021") %>%
  select(RECID, stub, stubnum, filer2017, type, agibase, usweight, wtype, iitax) %>%
  pivot_wider(names_from = type, values_from = iitax) %>%
  mutate(change=reform - base) %>%
  filter(change > 0)

increases %>% 
  arrange(-change)

increases %>%
  group_by(stubnum, stub, wtype) %>%
  summarise(n=n(), wtd=sum(usweight), wtdchange=sum(change * usweight), .groups="drop") %>%
  arrange(-wtdchange) 
  
```

## Compare base vs. reform in 2021 with puf weights and with reweights

The following tables show iitax, nationally, 2017 filers only, under base and reform, with default puf weights and with reweighted values, followed by the same kind of information for the taxes-paid deduction.

Note that:

-   Reweighted is about \$11b more expensive than puf weights
-   Essentially all of the difference is for millionaires. See the earlier discussion of millionaires - it seems pretty clear that puf weights have too little taxes paid deductions for millionaires.

### iitax results with puf weights and then with reweights

 \
Here are the puf-weighted values  \
 

```{r include=TRUE}

tabdata <- getvar(wtype="WT2021", stabbr="US", variable="iitax")
vartab(tabdata, "iitax: SALT cap repeal vs. 2021 current law, 2021 income levels, default puf weights")


```

 \
And here are the reweighted values:  \
 

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="US", variable="iitax")
vartab(tabdata, "iitax:  SALT cap repeal vs. 2021 current law, 2021 income levels, reweighted")


```

### Taxes paid deduction with puf weights and then with reweights

 \
Here are the puf-weighted values\
 \
 

```{r include=TRUE}

tabdata <- getvar(wtype="WT2021", stabbr="US", variable="salt")
vartab(tabdata, "Taxes-paid deduction: SALT cap repeal vs. 2021 current law, 2021 income levels, default puf weights")


```

 \
 \
Here are the reweighted values.  \
 

```{r include=TRUE}

tabdata <- getvar(wtype="REWT2021", stabbr="US", variable="salt")
vartab(tabdata, "Taxes-paid deduction: SALT cap repeal vs. 2021 current law, 2021 income levels, reweighted")


```

## Why do MS and KY have such large percentage changes under either set of national weights?

### Kentucky iitax then salt, with reweights

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="KY", variable="iitax")
vartab(tabdata, 
       "iitax: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)

```

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="KY", variable="salt")
vartab(tabdata, 
       "salt: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)

```

### Mississippi iitax then salt, with reweights

```{r include=TRUE}

# getvar(wtype="REWT2021", stabbr="MS", variable="iitax")
tabdata <- getvar(wtype="REWT2021", stabbr="MS", variable="iitax")
vartab(tabdata, 
       "iitax: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)

```

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="MS", variable="salt")
vartab(tabdata, 
       "salt: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)

```

### For comparison: New York iitax then salt, with reweights

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="NY", variable="iitax")
vartab(tabdata, 
       "iitax: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)

```

```{r include=TRUE}
tabdata <- getvar(wtype="REWT2021", stabbr="NY", variable="salt")
vartab(tabdata, 
       "salt: SALT cap repeal vs. 2021 current law, 2021 income levels, reweights",
       dollar_digits=3)
```

# Status and key issues:

-   Things are set up in python to rerun baseline and reform if needed. This would only be necessary if puf.csv changes (e.g., if duplicate records are addressed), if Tax-Calculator changes, or the reform file changes. All seem unlikely.

-   Changes to national or state weights don't require rerunning baseline or reform in Tax-Calculator.

-   Initial focus has been on looking for anomalies and pumping out a few basic numbers.

-   The key data-quality and calculation issues appear to be:

    -   National:

        -   SALT data

            -   What should the amount and distribution across agi ranges of the uncapped taxes-paid deduction look like. It seems pretty clear that the default puf must have too little of the deduction and way too little of it among millionaires. But why does the default come so close to TPC?

            -   I hope Frank S. has some useful comments on that. Maybe they have too little SALT deduction, also. I suspect that.

            -   One way to investigate a little bit further is to see how the puf national impact changes when moving from 2020 to 2021 (our year). If the puf number goes up a lot from 2020 to 2021, then perhaps the TPC number would go up a lot too, bringing it closer to the \$90 billion I get with the reweighted puf. I calculated salt-cap national repeal costs in 2020 and 2021 (in python, not in this R program) using all defaults for both years and got the following:

                Cost 2020(\$B) = -76.988183

                Cost 2021(\$B) = -80.178242 (same as Matt's number)

                \$b change cost = -3.19

                \% change cost = 4.14

                Unfortunately, this suggests that TPC's estimate won't grow a lot from 2020 to 2021 -- maybe only to \$80 billion -- and will remain well below the \$90 billion estimate with my reweights. So I think either (1) TPC has too little SALT-qualifying deduction, (2) I have too much with the reweights, or (3) perhaps (far less likely) there is something not right about how Tax-Calculator is calculating it. It is hard for me to see how I can have significantly too much salt deduction given that I hit 2017 on the head, and the growth to 2021 seems plausible.

            -   Is it possible that c18300 is not the right variable to target? According to the 2011 SOI puf documentation, the important variables are e18400 (income or general sales tax), e18500 (real estate taxes), and e18600 (other taxes). It does not have e18300, instead c18300 is calculated and put on puf.csv by TaxData. Per [Tax-Calculator](http://taxcalc.pslmodels.org/guide/output_vars.html#c18300), c18300 is "Sch A: State and local taxes plus real estate taxes deducted (component of pre-limitation c21060 total)". It certainly sounds right.

            -   I calculated 2011 law in 2011 using puf.csv and got \$473.675 b as the weighted sum for c18300

            -   According to the relevant [IRS table](https://www.irs.gov/pub/irs-soi/11in21id.xls) the total taxes paid deduction on taxable returns was \$434.8 billion (this is the concept I target), plus another \$30.3 billion on nontaxable returns (both in column BQ). That is close enough to the calculated PUF value to make me think that c18300 really is the right variable.

            -   So if c18300 is the right variable, and if the reweighted values in 2017 are accurate, and if the 2017 to 2021 growfactors are reasonable, it is hard to see how the data file for 2021 can be far off. And I have confidence in Tax-Calculator. That leads me to think that \$90 billion is plausible and that TPC may be too low.

        -   Tax-Calculator produces tax increases for some records when the SALT cap is repealed. It's not a lot of records or a lot of money but it seems hard to understand. Would like to understand.

    -   State: Why do KY and MS have such large percentage tax cuts from SALT-cap repeal?

        -   I looked at the QC reports for these states and it looks like I came quite close to the important Historical Table 2 targets.

        -   Could the targets themselves be bad? I need to investige more. If that turns out to be true, I may want to rerun the state weighting.

        -   However, I just stumbled on what I think is the reason. Toward the end there are summary reports for KY, MS, and, for comparison, NY.

            -   NY has larger % tax cuts than KY and MS in the high income ranges as we'd expect.
            -   But KY and MS have a lot of people who have negative baseline tax (presumably EITC and other refundable credits). As a result, for their totals, we count their tax reductions across all taxpayers as a percentage of their baseline tax across all taxpayers - with that baseline reduced by a lot of credits.
            -   It may be that their liability for negative taxpayers is a far larger share of total net liability than it is in NY and a lot of other states, in part because the other states have more high-liability taxpayers, and so the overall average is skewed in some low-liability states.
            -   If this proves to be the primary explanation, then there may be nothing wrong at all. It's just that we have a presentation problem and will have to figure out a good way to present this. Maybe one way to present will be to average per-return changes.

Details below support the points above.

# Methodology and data notes

## Constructing 2021 data for the nation

-   **Start with official 2021-07-20 puf.csv**

-   Per the PSL Demo Day discussion, create 2017 national file:

    -   Extrapolate to 2017 using all defaults (growfactors.csv, puf_weights.csv, puf_ratios.csv)

    -   Calculate 2017 agi, determine 2017 filer status

    -   Reweight filers to approximate IRS national values for 2017 by agi range (including esp. c18300, taxes paid deduction)

    -   Note that WITHOUT reweighting, puf.csv in 2017 with defaults:

        -   has about \$34b (5.5%) too little taxes paid deduction
        -   \$30b of that is for agi ten-millionaires (64% below what IRS shows)
        -   another \~\$10b is for millionaires in the \$1m-10m range, with non-millionaires somewhat above IRS

    -   As reweighted, 2017 file matches IRS values within small tolerances

-   **Move 2017 file to 2021**

    -   Create records object from the reweighted file with start_year=2017, adjust_ratios=None (because I believe reweighting addresses interest income)

    -   Advance to 2017, which means it uses default puf growfactors for per-return values, but not puf_ratios. (There is a slight oddity: I used puf_ratios to move from 2011 to 2017, but not 2017 to 2021. This results in minimal differences vs no puf_ratios at all and if I were redoing it I probably would drop puf_ratios altogether.)

        -   The default growfactors I looked at seem very plausible.
        -   Taxes-paid components are grown by ATXPY. Its total growth from 2017 to 2021 is about 14.4% and its cagr is 3.48%. Looks reasonable.

    -   Note that using Tax-Calculator to advance overwrites my 2017 weights with those in puf_weights.csv, but that's ok - I replace them later.

-   **Sanity checks to this point**

    -   I verified by spot-checking individual records that SALT components (e18400, etc.) grow by 14.4%.
    -   Using the method above (default growfactors and weights, no puf_ratios) produces a SALT deduction for all millionaires in 2021, with the SALT cap removed, of \$110.7 billion. The value for millionaire filers in 2017 (also uncapped) was \$127.7 billion, per note to Frank Sammartino. So \$110.7 billion seems way too small and different weights are needed. (I can't easily compare all records to IRS total because I would have to drop out nonfilers.)

-   **National weights for 2021**

    -   Get puf_weights for 2017 and 2021 (WT2017, WT2021)

    -   Retrieve and merge in my 2017 filer weights and a filer indicator

    -   Create REWT2017 with WT2017 for 2017 nonfilers and my 2017 weight for 2017 filers

    -   Create REWT2021 as follows:

        -   2017 nonfilers: use WT2021 (their TaxData weight)

        -   2017 filers: REWT2017 + 4.8%, the growth the sum of TaxData weights for the 2017 nonfilers -- this is a 1.18% cagr for the number of filers, which I think (??) is reasonable

            -   IRS projected growth in returns 2017-2021 was 5.38% [per Tax-Calculator](https://github.com/PSLmodels/taxdata/blob/master/puf_stage1/IRS_return_projection.csv) so this seems reasonable (within 0.6% over 4 years)
            -   IRS number of returns in 2021 is 161,707,000
            -   When I use these weights to calculate weighted number of 2017 filers in 2021, I get 160,227,908, 0.9% lower than IRS
            -   puf weights give 151,156,078, 6.5% lower than IRS
            -   a better comparison might be filers under 2021 rules...

## State weights

I apply my state shares for each 2017 filer record to the 2021 national weight for those records. Have not yet addressed 2017 nonfilers. For now, they drop out of the analysis.

## Some links to estimates

-   [TPC 2020 repeal \$76b](https://www.taxpolicycenter.org/taxvox/tpc-analyzes-five-ways-replace-salt-deduction-cap)
-   [also this](https://www.taxpolicycenter.org/publications/alternatives-tcja-limit-state-and-local-tax-deduction/full)
